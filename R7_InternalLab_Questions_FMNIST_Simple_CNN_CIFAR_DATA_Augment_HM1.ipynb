{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment-HM1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanthmeruga/IEApp/blob/master/R7_InternalLab_Questions_FMNIST_Simple_CNN_CIFAR_DATA_Augment_HM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MyfMmMnPJjvn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "zjcGOJhcJjvp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "metadata": {
        "id": "jR0Pl2XjJjvq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "metadata": {
        "id": "Qr75v_UYJjvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02a2606a-7db4-4402-806a-71583a642980"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import vis\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "I9RA_PgeOLHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e60829b8-2712-466b-c75a-0b7f96fa2958"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 8us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 4s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hTI42-0qJjvw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "metadata": {
        "id": "g2sf67VoJjvx",
        "colab_type": "code",
        "outputId": "e1a79c33-e846-4c33-e61f-dce943d75f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "zewyDcBlJjv1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WytT2eRnJjv4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "metadata": {
        "id": "XycQGBSGJjv5",
        "colab_type": "code",
        "outputId": "34f68434-03d4-4ed5-8bc8-2152f7f5a210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "5jtdZ7RqJjv8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "metadata": {
        "id": "3DWNbfQiRH14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_conv = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test_conv = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "y_train_class = keras.utils.to_categorical(y_train, 10)\n",
        "y_test_class = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VeSfPHbDL2Su",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAD3q5I6Jjv9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mgHSCXy3JjwA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xO5BRBzBJjwD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "metadata": {
        "id": "3fUQpMHxJjwE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_conv =  x_train_conv.astype(\"float32\") / 255\n",
        "x_test_conv = x_test_conv.astype(\"float32\") / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Okwo_SB5JjwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "da5-DwgrJjwM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "metadata": {
        "id": "LPGVQ-JJJjwN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_conv = x_train.reshape(x_train_conv.shape[0], 28, 28, 1)\n",
        "x_test_conv = x_test.reshape(x_test_conv.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OFRRTJq8JjwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "metadata": {
        "id": "dWTZYnKSJjwR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C18AoS7eJjwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "metadata": {
        "id": "DORCLgSwJjwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "fcf18f83-8b6d-467d-abd9-e638db127a09"
      },
      "cell_type": "code",
      "source": [
        "model_simple_conv = Sequential()\n",
        "model_simple_conv.add(Conv2D(32, (3, 3), activation =\"relu\", input_shape=(28, 28, 1)))\n",
        "model_simple_conv.add(Conv2D(32, (3, 3), activation =\"relu\"))\n",
        "model_simple_conv.add(Flatten())\n",
        "model_simple_conv.add(Dense(128, activation='relu'))\n",
        "model_simple_conv.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pdL6MCEdSlMQ",
        "colab_type": "code",
        "outputId": "57652553-be57-4a07-8df3-64d7252d400b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "model_simple_conv.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               2359424   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,370,282\n",
            "Trainable params: 2,370,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MptqZ9ZATFdf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_simple_conv.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIV5IiAEVr3W",
        "colab_type": "code",
        "outputId": "ba71920e-9049-4fde-b6eb-23e859a2ace4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "%%time \n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = [EarlyStopping(patience=5)]\n",
        "output_pooling_conv = model_simple_conv.fit(x_train_conv, y_train_class, batch_size=512, epochs=10, verbose=2, callbacks=early_stopping,\n",
        "                    validation_data=(x_test_conv, y_test_class))\n",
        "                              "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " - 9s - loss: 12.9158 - acc: 0.1971 - val_loss: 12.8977 - val_acc: 0.1998\n",
            "Epoch 2/10\n",
            " - 4s - loss: 12.9075 - acc: 0.1992 - val_loss: 12.9090 - val_acc: 0.1991\n",
            "Epoch 3/10\n",
            " - 4s - loss: 12.9068 - acc: 0.1992 - val_loss: 12.9090 - val_acc: 0.1991\n",
            "Epoch 4/10\n",
            " - 4s - loss: 12.9068 - acc: 0.1992 - val_loss: 12.9090 - val_acc: 0.1991\n",
            "Epoch 5/10\n",
            " - 4s - loss: 12.9068 - acc: 0.1992 - val_loss: 12.9090 - val_acc: 0.1991\n",
            "Epoch 6/10\n",
            " - 4s - loss: 12.9068 - acc: 0.1992 - val_loss: 12.9090 - val_acc: 0.1991\n",
            "CPU times: user 15.5 s, sys: 6.44 s, total: 22 s\n",
            "Wall time: 28.9 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ju69vKdIJjwX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "metadata": {
        "id": "L2hAP94vJjwY",
        "colab_type": "code",
        "outputId": "c32db58d-ee92-4a74-820c-5b4a1297194b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "model_pooling_conv = Sequential()\n",
        "model_pooling_conv.add(Conv2D(32, (3, 3), activation =\"relu\", input_shape=(28, 28, 1)))\n",
        "model_pooling_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_pooling_conv.add(Dropout(0.25))\n",
        "model_pooling_conv.add(Conv2D(32, (3, 3), activation =\"relu\"))\n",
        "model_pooling_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_pooling_conv.add(Dropout(0.25))\n",
        "model_pooling_conv.add(Flatten())\n",
        "model_pooling_conv.add(Dense(128, activation='relu'))\n",
        "model_pooling_conv.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LCQLXjZ4aGi9",
        "colab_type": "code",
        "outputId": "0bb48e1c-ac08-4946-bc5c-71349239f27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "model_pooling_conv.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jdBUsoskcqB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_pooling_conv.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3RT5zZq8bP0O",
        "colab_type": "code",
        "outputId": "74d5a214-2fa1-41cc-c949-a61faf6b3156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "%%time \n",
        "from keras.callbacks import EarlyStopping\n",
        "output_pooling_conv = model_pooling_conv.fit(x_train_conv, y_train_class, batch_size=512, epochs=10, verbose=2, callbacks=early_stopping,\n",
        "                    validation_data=(x_test_conv, y_test_class))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " - 3s - loss: 8.6098 - acc: 0.4481 - val_loss: 4.1248 - val_acc: 0.6325\n",
            "Epoch 2/10\n",
            " - 2s - loss: 1.1787 - acc: 0.7006 - val_loss: 0.6387 - val_acc: 0.7884\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.5965 - acc: 0.7796 - val_loss: 0.5227 - val_acc: 0.8143\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.5192 - acc: 0.8073 - val_loss: 0.4645 - val_acc: 0.8349\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.4703 - acc: 0.8264 - val_loss: 0.4159 - val_acc: 0.8498\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.4317 - acc: 0.8395 - val_loss: 0.3909 - val_acc: 0.8563\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.4068 - acc: 0.8498 - val_loss: 0.3671 - val_acc: 0.8597\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.3816 - acc: 0.8579 - val_loss: 0.3690 - val_acc: 0.8568\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.3666 - acc: 0.8654 - val_loss: 0.3385 - val_acc: 0.8752\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.3522 - acc: 0.8701 - val_loss: 0.3378 - val_acc: 0.8711\n",
            "CPU times: user 17.1 s, sys: 5.54 s, total: 22.6 s\n",
            "Wall time: 24.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3fbIHYEddKyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGTA3bfEJjwa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "metadata": {
        "id": "F6gX8n5SJjwb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "metadata": {
        "id": "Cbz4uHBuJjwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BlW1qIkyeFgd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jb0eQHuKeaXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen.fit(x_train_conv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pl-8dOo7Jjwf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "DpI1_McYJjwg",
        "colab_type": "code",
        "outputId": "da260c99-01c4-486f-d410-e0c078c4cd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train_conv[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dmPl5yE8Jjwm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "metadata": {
        "id": "44ZnDdJYJjwn",
        "colab_type": "code",
        "outputId": "d5e8d791-92e4-4a8f-b001-3e472bc941cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "x_train_conv[:1].shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "IKG_BeA8fHZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen.fit(x_train_conv[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1_qSLv9ifbsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = datagen.flow(x_train_conv[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5amaPBG_fP0C",
        "colab_type": "code",
        "outputId": "a5b5a335-6d0c-472a-daa9-53b7969c1605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "image = []\n",
        "for i in range(3):\n",
        "    img = samples.next()\n",
        "    img = img.squeeze()\n",
        "    image.append(vis.imshow(img))\n",
        "image[0] | image[1] | image[2]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-de9bc44b3743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'vis' has no attribute 'imshow'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "MwQQW5iOJjwq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "c1SrtBEPJjwq",
        "colab_type": "code",
        "outputId": "109c5414-38b5-4ba2-a277-de1b23ded767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "model_pooling_conv.evaluate(x_test_conv,y_test_class)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 71us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33777024414539336, 0.8711]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "ZBwVWNQC2qZD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KXqmUDW2rM1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## **DATA AUGMENTATION ON CIFAR10 DATASET**"
      ]
    },
    {
      "metadata": {
        "id": "8mja6OgQ3L18",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One of the best ways to improve the performance of a Deep Learning model is to add more data to the training set. Aside from gathering more instances from the wild that are representative of the distinction task, we want to develop a set of methods that enhance the data we already have. There are many ways to augment existing datasets and produce more robust models. In the image domain, these are done to utilize the full power of the convolutional neural network, which is able to capture translational invariance. This translational invariance is what makes image recognition such a difficult task in the first place. You want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions that are of interest to the vision task."
      ]
    },
    {
      "metadata": {
        "id": "6HzVTPUM3WZJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Import neessary libraries for data augmentation**"
      ]
    },
    {
      "metadata": {
        "id": "PPM558TX4KMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6hicLwP4SqY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Load CIFAR10 dataset**"
      ]
    },
    {
      "metadata": {
        "id": "NQ1WzrXd4WNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cc996f24-3c3f-4a3c-8c27-54581f6ba6cf"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 31s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R9Pht1ggHuiT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3n28ccU6Hp6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JN3vYYhK4W0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Create a data_gen funtion to genererator with image rotation,shifting image horizontally and vertically with random flip horizontally.**"
      ]
    },
    {
      "metadata": {
        "id": "JJbekTKi4cmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_gen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-SLtUhC4dK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Prepare/fit the generator.**"
      ]
    },
    {
      "metadata": {
        "id": "CSw8Bv2_4hb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gYyF-P8O4jQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **Generate 5 images for 1 of the image of CIFAR10 train dataset.**"
      ]
    },
    {
      "metadata": {
        "id": "mXug4z234mwQ",
        "colab_type": "code",
        "outputId": "08c159ab-8857-4d42-cd50-dadd990fca1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "gen = datagen.flow(x_train_conv[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTBD+sAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fZHmUPqZjOiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}